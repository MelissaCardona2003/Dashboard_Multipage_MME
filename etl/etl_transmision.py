#!/usr/bin/env python3
"""
ETL para L√≠neas de Transmisi√≥n
Descarga datos desde SIMEM API y los almacena en SQLite

Uso:
    python3 etl/etl_transmision.py [--days DAYS] [--clean]
    
Opciones:
    --days DAYS    D√≠as hacia atr√°s para descargar (default: 7)
    --clean        Limpia datos antiguos (>90 d√≠as)
"""

import sys
import os
from datetime import datetime, timedelta
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from pydataxm.pydatasimem import ReadSIMEM
import pandas as pd
from infrastructure.database.repositories.transmission_repository import TransmissionRepository


def fetch_transmission_data(days_back: int = 7) -> pd.DataFrame:
    """
    Descarga datos de transmisi√≥n desde SIMEM API
    
    Args:
        days_back: D√≠as hacia atr√°s para descargar
        
    Returns:
        DataFrame con datos de l√≠neas
    """
    try:
        end_date = datetime.now().strftime('%Y-%m-%d')
        start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')
        
        print(f"üì° Descargando datos SIMEM desde {start_date} hasta {end_date}...")
        print("üîÑ Dataset: 7538fd (Par√°metros t√©cnicos de l√≠neas de transmisi√≥n)")
        
        # Dataset 7538fd: Par√°metros t√©cnicos de l√≠neas de transmisi√≥n
        reader = ReadSIMEM('7538fd', start_date, end_date)
        df = reader.main()
        
        if df is None or df.empty:
            print("‚ö†Ô∏è SIMEM API retorn√≥ DataFrame vac√≠o")
            return pd.DataFrame()
        
        print(f"‚úÖ Descargados {len(df)} registros")
        
        # Convertir fechas
        df['Fecha'] = pd.to_datetime(df['Fecha'])
        df['FechaPublicacion'] = pd.to_datetime(df['FechaPublicacion'])
        if 'FPO' in df.columns:
            df['FPO'] = pd.to_datetime(df['FPO'], errors='coerce')
        
        return df
        
    except Exception as e:
        print(f"‚ùå Error descargando datos SIMEM: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()


def run_etl(days_back: int = 7, clean_old: bool = False):
    """
    Ejecuta el proceso ETL completo
    
    Args:
        days_back: D√≠as hacia atr√°s para descargar
        clean_old: Si es True, elimina datos antiguos
    """
    print("=" * 60)
    print("ETL TRANSMISI√ìN - L√≠neas de Transmisi√≥n Nacional")
    print("=" * 60)
    print(f"Inicio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # 1. Descargar datos
    df = fetch_transmission_data(days_back)
    
    if df.empty:
        print("‚ùå No se pudieron descargar datos. ETL abortado.")
        return False
    
    # 2. Insertar en DB
    try:
        repo = TransmissionRepository()
        
        print(f"üíæ Insertando {len(df)} registros en SQLite...")
        inserted = repo.bulk_insert_lines(df)
        print(f"‚úÖ Insertados {inserted} registros nuevos (duplicados omitidos)")
        
        # 3. Limpieza (opcional)
        if clean_old:
            print("üßπ Limpiando datos antiguos (>90 d√≠as)...")
            deleted = repo.delete_old_data(days_to_keep=90)
            print(f"‚úÖ Eliminados {deleted} registros antiguos")
        
        # 4. Estad√≠sticas finales
        total_lines = repo.get_total_lines()
        latest_date = repo.get_latest_date()
        
        print()
        print("=" * 60)
        print("RESUMEN")
        print("=" * 60)
        print(f"Total l√≠neas √∫nicas en DB: {total_lines}")
        print(f"Fecha m√°s reciente: {latest_date}")
        print(f"Fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error en ETL: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ETL para L√≠neas de Transmisi√≥n')
    parser.add_argument('--days', type=int, default=7, help='D√≠as hacia atr√°s para descargar (default: 7)')
    parser.add_argument('--clean', action='store_true', help='Limpia datos antiguos (>90 d√≠as)')
    
    args = parser.parse_args()
    
    success = run_etl(days_back=args.days, clean_old=args.clean)
    sys.exit(0 if success else 1)
