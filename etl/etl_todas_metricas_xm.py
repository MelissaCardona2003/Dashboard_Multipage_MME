#!/usr/bin/env python3
"""
ETL COMPLETO: Descarga TODAS las mÃ©tricas de XM (193 mÃ©tricas)
================================================================

Este script consulta la API de XM, obtiene la lista completa de mÃ©tricas
disponibles y las descarga todas a la base de datos SQLite.

Uso:
    python3 etl/etl_todas_metricas_xm.py [--dias 90] [--solo-nuevas]
    
Argumentos:
    --dias: NÃºmero de dÃ­as de historia (default: 90)
    --solo-nuevas: Solo descargar mÃ©tricas que no estÃ¡n en BD
    --metrica: Descargar solo una mÃ©trica especÃ­fica
    --seccion: Descargar solo mÃ©tricas de una secciÃ³n especÃ­fica
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pydataxm.pydataxm import ReadDB
from datetime import datetime, timedelta
import time
import logging
import pandas as pd
import argparse
import sqlite3
from utils import db_manager

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

DB_PATH = '/home/admonctrlxm/server/portal_energetico.db'

# ClasificaciÃ³n de mÃ©tricas por secciÃ³n
METRICAS_POR_SECCION = {
    'GeneraciÃ³n': ['Gene', 'GeneIdea', 'GeneProgDesp', 'GeneProgRedesp', 'GeneFueraMerito', 
                   'GeneSeguridad', 'CapEfecNeta', 'ENFICC', 'ObligEnerFirme', 'DDVContratada',
                   'CapaTeoHidroNacion', 'CapaEfecPorRecDesp', 'CapaDispoReduObli'],
    'Demanda': ['DemaReal', 'DemaCome', 'DemaRealReg', 'DemaRealNoReg', 'DemaComeReg', 
                'DemaComeNoReg', 'DemaSIN', 'DemaMaxPot', 'DemaNoAtenProg', 'DemaNoAtenNoProg', 
                'DemaOR', 'DemaNOOR', 'DemaProgRegu', 'DemaProgNoRegu', 'DemaTotalBolsa',
                'RecuMeReguMora', 'RecuMeNoReguMora', 'RecuMeMoraTotal', 'GranConsPrecRegu',
                'GranConsPromNoRegu', 'ValorDemandaProgDesp'],
    'TransmisiÃ³n': ['DispoReal', 'DispoCome', 'DispoDeclarada', 'CargoUsoSTN', 'CargoUsoSTR'],
    'Restricciones': ['RestAliv', 'RestSinAliv', 'RentasCongestRestr', 'EjecGarantRestr', 
                      'DesvGenVariableDesp', 'DesvGenVariableRedesp'],
    'Precios': ['PrecBolsNaci', 'PrecBolsNaciTX1', 'PPPrecBolsNaci', 'PrecTransBolsa',
                'PrecPromCont', 'PrecPromContRegu', 'PrecPromContNoRegu',
                'PrecEsca', 'PrecEscaAct', 'PrecEscaMarg', 'PrecEscaPon',
                'PrecOferDesp', 'PrecOferIdeal', 'MaxPrecOferNal',
                'CostMargDesp', 'CostRecPos', 'CostRecNeg', 'PrecCargConf',
                'PrecPromBolsAgen', 'PromPondPrecBolsNaci', 'PrecDespIdealTX1',
                'PrecNudoCont', 'PrecContDeclaTX1'],
    'Transacciones': ['CompBolsNaciEner', 'VentBolsNaciEner', 'CompContEner', 'VentContEner',
                      'CompBolsaTIEEner', 'VentBolsaTIEEner', 'CompBolsaIntEner', 'VentBolsaIntEner',
                      'CompAcumBolsaNaci', 'VentAcumBolsaNaci', 'CompAcumBolsaTIE', 'VentAcumBolsaTIE',
                      'CompAcumBolsaInt', 'VentAcumBolsaInt', 'TransacFrontera', 'LiqContBilateral',
                      'IngresosContrato', 'CompContDeclaTX1', 'VentContDeclaTX1', 'TransInternNaci',
                      'CompNudoCont', 'VentNudoCont', 'CompGenCont', 'VentGenCont'],
    'PÃ©rdidas': ['PerdidasEner', 'PerdidasEnerReg', 'PerdidasEnerNoReg',
                 'CompPerdiEner', 'CompPerdiReg', 'CompPerdiNoReg'],
    'Intercambios': ['ImpoEner', 'ExpoEner', 'SnTIEMerito', 'DeltaInt', 'ImpoMerito',
                     'ExpoMerito', 'ImpoCapacidad', 'ExpoCapacidad', 'TransFrontera',
                     'CapaTotalTIE', 'ImpoProgrTIE', 'ExpoProgrTIE', 'ImpoRealTIE', 'ExpoRealTIE', 'CapaDispoTIE'],
    'HidrologÃ­a': ['AporEner', 'VoluUtilDiarEner', 'CapaUtilDiarEner', 'VertEner',
                   'AporValorEner', 'VoluFinalMensEner', 'EneIndisp',
                   'AportHidricoMens', 'VolUtilesMens', 'VolUtilAgre',
                   'AportPorRecur', 'VolUtilPorRecur', 'MediaHist', 'PromediosAlDia',
                   'SeriesHistAport', 'AporMedioBasin', 'VolUtilBasin', 'AporAfluen',
                   'VolUtilAfluen', 'AporMedioAfluen', 'VolMedioAfluen', 'CotaEmbalse', 'NivelRio'],
    'Combustibles': ['ConsCombustibleMBTU', 'EmisionesCO2', 'factorEmisionCO2e',
                     'ConsGasKPCE', 'ConsCarbon', 'ConsJetA1', 'ConsFuelOil', 'ConsGasNatural'],
    'Renovables': ['IrrPanel', 'IrrGlobal', 'TempPanel', 'TempAmbSolar'],
    'Cargos': ['FAZNI', 'FAER', 'PRONE', 'CargoUsoSTN', 'CargoUsoSTR',
               'CargMaxTPrima', 'CargDistribu', 'CargComer', 'CargRestric',
               'CargConfiabili', 'CargAGC']
}

def obtener_todas_metricas_xm(obj_api):
    """Obtener lista completa de mÃ©tricas disponibles en XM"""
    logging.info("ğŸ“¡ Consultando lista completa de mÃ©tricas en API XM...")
    
    try:
        df_metricas = obj_api.all_variables()
        logging.info(f"âœ… Encontradas {len(df_metricas)} mÃ©tricas en XM")
        return df_metricas
    except Exception as e:
        logging.error(f"âŒ Error al consultar mÃ©tricas: {e}")
        return None

def obtener_metricas_en_bd():
    """Obtener lista de mÃ©tricas que ya estÃ¡n en la base de datos"""
    try:
        conn = sqlite3.connect(DB_PATH)
        query = "SELECT DISTINCT metrica FROM metrics"
        df = pd.read_sql_query(query, conn)
        conn.close()
        return set(df['metrica'].tolist())
    except Exception as e:
        logging.error(f"âŒ Error al consultar BD: {e}")
        return set()

def detectar_conversion(metric_id, entity):
    """Detectar tipo de conversiÃ³n necesaria basado en el nombre de la mÃ©trica"""
    # HidrologÃ­a - datos en Wh
    if metric_id in ['AporEner', 'VoluUtilDiarEner', 'CapaUtilDiarEner', 'VertEner', 
                     'AporValorEner', 'VoluFinalMensEner', 'EneIndisp']:
        return 'Wh_a_GWh'
    
    # Disponibilidad - promedio horario
    if 'Dispo' in metric_id:
        return 'horas_a_MW'
    
    # GeneraciÃ³n - suma horaria
    if 'Gene' in metric_id or metric_id in ['CapEfecNeta', 'CapaTeoHidroNacion']:
        return 'horas_a_GWh'
    
    # Demanda - suma horaria
    if 'Dema' in metric_id:
        return 'horas_a_GWh'
    
    # Precios, cargos - sin conversiÃ³n
    if 'Prec' in metric_id or 'Cargo' in metric_id or 'Cost' in metric_id:
        return 'sin_conversion'
    
    # Transacciones - suma horaria generalmente
    if 'Comp' in metric_id or 'Vent' in metric_id or 'Trans' in metric_id:
        return 'horas_a_GWh'
    
    # Por defecto
    return 'sin_conversion'

def convertir_unidades(df, metric, conversion_type):
    """Convertir unidades de datos crudos de XM"""
    if df is None or df.empty:
        return df
    
    df = df.copy()
    
    try:
        if conversion_type == 'Wh_a_GWh':
            if 'Value' in df.columns:
                df['Value'] = df['Value'] / 1_000_000
                logging.info(f"  âœ… Convertido Wh â†’ GWh")
        
        elif conversion_type == 'kWh_a_GWh':
            if 'Value' in df.columns:
                df['Value'] = df['Value'] / 1_000_000
                logging.info(f"  âœ… Convertido kWh â†’ GWh")
        
        elif conversion_type == 'horas_a_MW':
            # Disponibilidad: Promediar valores horarios
            hour_cols = [f'Values_Hour{i:02d}' for i in range(1, 25)]
            existing_cols = [col for col in hour_cols if col in df.columns]
            
            if existing_cols:
                df['Value'] = df[existing_cols].mean(axis=1) / 1_000  # kW â†’ MW
                df = df.dropna(subset=['Value'])
                logging.info(f"  âœ… Promediado {len(existing_cols)} horas â†’ MW")
            elif 'Value' in df.columns:
                df['Value'] = df['Value'] / 1_000
                logging.info(f"  âœ… Convertido kW â†’ MW")
        
        elif conversion_type == 'horas_a_GWh':
            # GeneraciÃ³n/Demanda: Sumar valores horarios
            hour_cols = [f'Values_Hour{i:02d}' for i in range(1, 25)]
            existing_cols = [col for col in hour_cols if col in df.columns]
            
            if existing_cols:
                df['Value'] = df[existing_cols].sum(axis=1) / 1_000_000  # kWh â†’ GWh
                df = df.dropna(subset=['Value'])
                logging.info(f"  âœ… Sumado {len(existing_cols)} horas â†’ GWh")
            elif 'Value' in df.columns:
                df['Value'] = df['Value'] / 1_000_000
                logging.info(f"  âœ… Convertido kWh â†’ GWh")
        
    except Exception as e:
        logging.warning(f"  âš ï¸ Error en conversiÃ³n: {e}")
    
    return df

def descargar_metrica(obj_api, metric_id, entity, dias_historia=90):
    """Descargar una mÃ©trica especÃ­fica de XM"""
    fecha_fin = datetime.now() - timedelta(days=1)
    fecha_inicio = fecha_fin - timedelta(days=dias_historia)
    
    logging.info(f"\n{'='*70}")
    logging.info(f"ğŸ“Š MÃ©trica: {metric_id} | Entidad: {entity}")
    logging.info(f"ğŸ“… PerÃ­odo: {fecha_inicio.date()} â†’ {fecha_fin.date()}")
    
    try:
        # Consultar API
        df = obj_api.request_data(
            metric_id,
            entity,
            start_date=fecha_inicio.strftime('%Y-%m-%d'),
            end_date=fecha_fin.strftime('%Y-%m-%d')
        )
        
        if df is None or df.empty:
            logging.warning(f"  âš ï¸ Sin datos disponibles")
            return 0
        
        logging.info(f"  âœ… Descargados {len(df)} registros")
        
        # Detectar y aplicar conversiÃ³n
        conversion = detectar_conversion(metric_id, entity)
        df = convertir_unidades(df, metric_id, conversion)
        
        if df.empty:
            logging.warning(f"  âš ï¸ Sin datos despuÃ©s de conversiÃ³n")
            return 0
        
        # Preparar datos para inserciÃ³n
        registros = []
        
        # Detectar columnas relevantes
        fecha_col = 'Date' if 'Date' in df.columns else 'date'
        valor_col = 'Value'
        
        # Columnas de identificaciÃ³n (priorizar Name sobre Id)
        id_cols = []
        if 'Name' in df.columns:
            id_cols.append('Name')
        elif 'Code' in df.columns:
            id_cols.append('Code')
        elif 'Agent' in df.columns:
            id_cols.append('Agent')
        elif 'Id' in df.columns:
            id_cols.append('Id')
        
        for _, row in df.iterrows():
            # Recurso/Agente/Id
            recurso = None
            if id_cols:
                recurso = str(row[id_cols[0]]) if pd.notna(row[id_cols[0]]) else None
            
            # Fecha
            fecha = pd.to_datetime(row[fecha_col]).strftime('%Y-%m-%d')
            
            # Valor
            valor = float(row[valor_col]) if pd.notna(row[valor_col]) else None
            
            if valor is not None:
                registros.append({
                    'fecha': fecha,
                    'metrica': metric_id,
                    'entidad': entity,
                    'recurso': recurso,
                    'valor_gwh': valor
                })
        
        # Insertar en BD
        if registros:
            conn = sqlite3.connect(DB_PATH)
            cursor = conn.cursor()
            
            for reg in registros:
                cursor.execute("""
                    INSERT OR REPLACE INTO metrics 
                    (fecha, metrica, entidad, recurso, valor_gwh, fecha_actualizacion)
                    VALUES (?, ?, ?, ?, ?, datetime('now'))
                """, (reg['fecha'], reg['metrica'], reg['entidad'], 
                      reg['recurso'], reg['valor_gwh']))
            
            conn.commit()
            conn.close()
            
            logging.info(f"  ğŸ’¾ Insertados {len(registros)} registros en BD")
            return len(registros)
        else:
            logging.warning(f"  âš ï¸ No hay registros para insertar")
            return 0
            
    except Exception as e:
        logging.error(f"  âŒ Error: {e}")
        return 0

def ejecutar_etl_completo(dias=90, solo_nuevas=False, metrica_especifica=None, seccion_especifica=None):
    """Ejecutar ETL completo de todas las mÃ©tricas"""
    inicio = time.time()
    stats = {
        'total': 0,
        'exitosas': 0,
        'fallidas': 0,
        'sin_datos': 0,
        'registros': 0
    }
    
    logging.info("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    logging.info("â•‘     ETL COMPLETO - TODAS LAS MÃ‰TRICAS XM â†’ SQLite           â•‘")
    logging.info("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    logging.info(f"ğŸ“… DÃ­as de historia: {dias}")
    logging.info(f"ğŸ”„ Solo nuevas: {'SÃ­' if solo_nuevas else 'No'}")
    
    # Conectar a API
    logging.info("\nğŸ”Œ Conectando a API XM...")
    obj_api = ReadDB()
    
    # Obtener lista completa de mÃ©tricas
    df_metricas = obtener_todas_metricas_xm(obj_api)
    if df_metricas is None:
        logging.error("âŒ No se pudo obtener lista de mÃ©tricas")
        return
    
    # Filtrar por mÃ©trica especÃ­fica
    if metrica_especifica:
        df_metricas = df_metricas[df_metricas['MetricId'] == metrica_especifica]
        logging.info(f"ğŸ¯ Filtrando por mÃ©trica: {metrica_especifica}")
    
    # Filtrar por secciÃ³n
    if seccion_especifica and seccion_especifica in METRICAS_POR_SECCION:
        metricas_seccion = METRICAS_POR_SECCION[seccion_especifica]
        df_metricas = df_metricas[df_metricas['MetricId'].isin(metricas_seccion)]
        logging.info(f"ğŸ“‚ Filtrando por secciÃ³n: {seccion_especifica} ({len(df_metricas)} mÃ©tricas)")
    
    # Obtener mÃ©tricas ya en BD
    if solo_nuevas:
        metricas_bd = obtener_metricas_en_bd()
        logging.info(f"ğŸ“Š MÃ©tricas ya en BD: {len(metricas_bd)}")
        df_metricas = df_metricas[~df_metricas['MetricId'].isin(metricas_bd)]
        logging.info(f"ğŸ†• MÃ©tricas nuevas a descargar: {len(df_metricas)}")
    
    stats['total'] = len(df_metricas)
    
    # Procesar cada mÃ©trica
    for idx, row in df_metricas.iterrows():
        metric_id = row['MetricId']
        entity = row['Entity']
        
        registros = descargar_metrica(obj_api, metric_id, entity, dias)
        
        if registros > 0:
            stats['exitosas'] += 1
            stats['registros'] += registros
        elif registros == 0:
            stats['sin_datos'] += 1
        else:
            stats['fallidas'] += 1
        
        # Pausa entre mÃ©tricas
        time.sleep(0.5)
    
    # Resumen
    tiempo_total = time.time() - inicio
    
    logging.info("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    logging.info("â•‘                    RESUMEN ETL COMPLETO                      â•‘")
    logging.info("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    logging.info(f"ğŸ“Š Total mÃ©tricas procesadas: {stats['total']}")
    logging.info(f"  âœ… Exitosas (con datos): {stats['exitosas']}")
    logging.info(f"  âš ï¸  Sin datos: {stats['sin_datos']}")
    logging.info(f"  âŒ Fallidas: {stats['fallidas']}")
    logging.info(f"ğŸ’¾ Total registros insertados: {stats['registros']:,}")
    logging.info(f"â±ï¸  Tiempo total: {tiempo_total:.1f} seg ({tiempo_total/60:.1f} min)")
    
    # EstadÃ­sticas de BD
    try:
        conn = sqlite3.connect(DB_PATH)
        df_stats = pd.read_sql_query("""
            SELECT 
                COUNT(*) as total_registros,
                COUNT(DISTINCT metrica) as metricas_unicas,
                COUNT(DISTINCT fecha) as dias_unicos,
                MIN(fecha) as fecha_min,
                MAX(fecha) as fecha_max
            FROM metrics
        """, conn)
        conn.close()
        
        logging.info(f"\nğŸ“ˆ EstadÃ­sticas de Base de Datos:")
        logging.info(f"  Total registros: {df_stats['total_registros'][0]:,}")
        logging.info(f"  MÃ©tricas Ãºnicas: {df_stats['metricas_unicas'][0]}")
        logging.info(f"  DÃ­as Ãºnicos: {df_stats['dias_unicos'][0]}")
        logging.info(f"  Rango: {df_stats['fecha_min'][0]} â†’ {df_stats['fecha_max'][0]}")
    except Exception as e:
        logging.error(f"âŒ Error al obtener estadÃ­sticas: {e}")
    
    logging.info(f"\nâœ… ETL completado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ETL Completo: Todas las mÃ©tricas XM')
    parser.add_argument('--dias', type=int, default=90, help='DÃ­as de historia (default: 90)')
    parser.add_argument('--solo-nuevas', action='store_true', help='Solo descargar mÃ©tricas nuevas')
    parser.add_argument('--metrica', type=str, help='Descargar solo una mÃ©trica especÃ­fica')
    parser.add_argument('--seccion', type=str, help='Descargar solo mÃ©tricas de una secciÃ³n',
                       choices=list(METRICAS_POR_SECCION.keys()))
    
    args = parser.parse_args()
    
    ejecutar_etl_completo(
        dias=args.dias,
        solo_nuevas=args.solo_nuevas,
        metrica_especifica=args.metrica,
        seccion_especifica=args.seccion
    )
