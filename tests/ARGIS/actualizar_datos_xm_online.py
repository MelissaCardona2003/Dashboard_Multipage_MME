#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script: Actualizaci√≥n autom√°tica de datos XM en ArcGIS Online
Usuario ArcGIS: melissacardona2
Fecha: 2026-02-06

Descripci√≥n:
    Extrae datos diarios de XM Colombia (Generaci√≥n, Precio de Bolsa, Volumen de Embalses)
    y los publica/actualiza en ArcGIS Online para visualizaci√≥n en dashboards.

Uso:
    python actualizar_datos_xm_online.py [--dry-run]

Requisitos:
    pip install pydataxm arcgis pandas python-dotenv
"""

import sys
import os
import logging
import argparse
from datetime import datetime, timedelta
from pathlib import Path
import pandas as pd
from pydataxm.pydataxm import ReadDB
from arcgis.gis import GIS
from arcgis.geometry import Point
from arcgis.features import FeatureLayer, GeoAccessor, GeoSeriesAccessor

# Intentar cargar variables de entorno desde .env
try:
    from dotenv import load_dotenv
    env_path = Path(__file__).parent / '.env'
    load_dotenv(dotenv_path=env_path)
except ImportError:
    pass

# ============================================
# CONFIGURACI√ìN - CREDENCIALES
# ============================================

# IMPORTANTE: Por seguridad, usa variables de entorno
# Crea un archivo .env en el mismo directorio con:
# ARCGIS_PORTAL_URL=https://arcgisenterprise.minenergia.gov.co/portal/
# ARCGIS_USERNAME=Vice_Energia
# ARCGIS_PASSWORD=Survey123+
# FEATURE_LAYER_ID=id_obtenido_primera_vez

ARCGIS_PORTAL_URL = os.getenv("ARCGIS_PORTAL_URL", "https://arcgisenterprise.minenergia.gov.co/portal/")
ARCGIS_USERNAME = os.getenv("ARCGIS_USERNAME", "Vice_Energia")
ARCGIS_PASSWORD = os.getenv("ARCGIS_PASSWORD", "Survey123+")

# ID del Feature Layer (obtenido despu√©s de la primera ejecuci√≥n)
FEATURE_LAYER_ID = os.getenv("FEATURE_LAYER_ID", None)

# Configuraci√≥n de fechas (d√≠as hacia atr√°s para extraer)
DIAS_ATRAS = int(os.getenv("DIAS_ATRAS", "7"))

# Ubicaci√≥n geogr√°fica (Bogot√° - Centro de Colombia)
BOGOTA_COORDS = {"x": -74.0721, "y": 4.7110, "spatialReference": {"wkid": 4326}}

# Configuraci√≥n de logging y archivos
BASE_DIR = Path(__file__).parent.parent.parent  # /home/admonctrlxm/server
LOG_DIR = BASE_DIR / "logs"
LOG_FILE = LOG_DIR / "actualizacion_xm_arcgis.log"

# Directorio para guardar copia permanente del CSV (sin backups)
DATA_DIR = BASE_DIR / "data"
CSV_OUTPUT_FILE = DATA_DIR / "metricas_xm_arcgis.csv"

# ============================================
# CONFIGURAR LOGGING Y DIRECTORIOS
# ============================================

LOG_DIR.mkdir(parents=True, exist_ok=True)
DATA_DIR.mkdir(parents=True, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

# ============================================
# FUNCIONES
# ============================================

def validar_credenciales():
    """Valida que las credenciales est√©n configuradas"""
    if not ARCGIS_USERNAME or not ARCGIS_PASSWORD:
        logger.error("‚ùå Credenciales no configuradas. Configura las variables de entorno:")
        logger.error("   ARCGIS_USERNAME y ARCGIS_PASSWORD")
        raise ValueError("Credenciales de ArcGIS no configuradas")


def verificar_datos_nuevos():
    """Verifica si hay datos nuevos disponibles en XM comparando con el CSV local"""
    try:
        if not CSV_OUTPUT_FILE.exists():
            logger.info("üìÑ No existe CSV previo. Se extraer√°n todos los datos.")
            return True, None
        
        # Leer CSV existente y obtener la √∫ltima fecha
        df_existente = pd.read_csv(CSV_OUTPUT_FILE)
        if len(df_existente) == 0:
            logger.info("üìÑ CSV existente est√° vac√≠o. Se extraer√°n datos.")
            return True, None
        
        ultima_fecha_str = df_existente['Fecha'].max()
        ultima_fecha = pd.to_datetime(ultima_fecha_str).date()
        
        # Verificar si hay datos m√°s recientes disponibles en XM
        # Probar desde la fecha siguiente a la √∫ltima hasta ayer
        fecha_probar = ultima_fecha + timedelta(days=1)
        fecha_ayer = (datetime.now() - timedelta(days=1)).date()
        
        logger.info(f"üìÖ √öltima fecha en CSV: {ultima_fecha}")
        logger.info(f"üîç Verificando disponibilidad de datos desde {fecha_probar} hasta {fecha_ayer}...")
        
        if fecha_probar > fecha_ayer:
            logger.info(f"‚ÑπÔ∏è  CSV ya est√° actualizado hasta {ultima_fecha}. No hay fechas nuevas que verificar.")
            return False, ultima_fecha
        
        # Verificar r√°pidamente si hay datos nuevos
        api = ReadDB()
        fecha_probar_str = fecha_probar.strftime('%Y-%m-%d')
        
        try:
            datos_test = api.request_data('Gene', 'Sistema', start_date=fecha_probar_str, end_date=fecha_probar_str)
            if datos_test is not None and len(datos_test) > 0:
                logger.info(f"‚úÖ Datos nuevos disponibles desde {fecha_probar}!")
                return True, ultima_fecha
            else:
                logger.info(f"‚è≥ No hay datos nuevos disponibles a√∫n. √öltima fecha: {ultima_fecha}")
                return False, ultima_fecha
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Error verificando datos: {e}. Continuando con actualizaci√≥n...")
            return True, ultima_fecha
            
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Error leyendo CSV existente: {e}. Continuando con actualizaci√≥n completa...")
        return True, None


#LO IMPORTANTE PARA LUISA
def extraer_datos_xm():
    """Extrae datos de la API XM"""
    try:
        logger.info("üöÄ Iniciando extracci√≥n de datos de API XM...")
        
        api = ReadDB()
        
        # Obtener fechas recientes (como en el ETL del proyecto)
        fecha_fin = datetime.now() - timedelta(days=1)  # Ayer
        fecha_inicio = fecha_fin - timedelta(days=DIAS_ATRAS)
        
        fecha_inicio_str = fecha_inicio.strftime('%Y-%m-%d')
        fecha_fin_str = fecha_fin.strftime('%Y-%m-%d')
        
        logger.info(f"üìÖ Rango: {fecha_inicio_str} a {fecha_fin_str}")
        
        # Extraer m√©tricas con manejo de errores individual
        metricas = {}
        
        try:
            logger.info("‚è≥ Extrayendo Generaci√≥n Total del Sistema...")
            metricas['generacion'] = api.request_data(
                'Gene',
                'Sistema',
                start_date=fecha_inicio_str,
                end_date=fecha_fin_str
            )
            logger.info(f"   ‚úì Generaci√≥n: {len(metricas['generacion'])} registros")
        except Exception as e:
            logger.error(f"   ‚úó Error extrayendo Generaci√≥n: {e}")
            raise
        
        try:
            logger.info("‚è≥ Extrayendo Precio de Bolsa Nacional...")
            metricas['precio'] = api.request_data(
                'PrecBolsNaci',
                'Sistema',
                start_date=fecha_inicio_str,
                end_date=fecha_fin_str
            )
            logger.info(f"   ‚úì Precio: {len(metricas['precio'])} registros")
        except Exception as e:
            logger.error(f"   ‚úó Error extrayendo Precio: {e}")
            raise
        
        try:
            logger.info("‚è≥ Extrayendo Volumen √ötil de Embalses...")
            metricas['volumen'] = api.request_data(
                'VoluUtilDiarEner',
                'Sistema',
                start_date=fecha_inicio_str,
                end_date=fecha_fin_str
            )
            logger.info(f"   ‚úì Volumen: {len(metricas['volumen'])} registros")
        except Exception as e:
            logger.error(f"   ‚úó Error extrayendo Volumen: {e}")
            raise
        
        try:
            logger.info("‚è≥ Extrayendo Capacidad √ötil de Embalses...")
            metricas['capacidad'] = api.request_data(
                'CapaUtilDiarEner',
                'Sistema',
                start_date=fecha_inicio_str,
                end_date=fecha_fin_str
            )
            logger.info(f"   ‚úì Capacidad: {len(metricas['capacidad'])} registros")
        except Exception as e:
            logger.error(f"   ‚úó Error extrayendo Capacidad: {e}")
            raise
        
        # Validar que se extrajeron datos
        for nombre, df in metricas.items():
            if df is None or len(df) == 0:
                raise ValueError(f"No se extrajeron datos para {nombre}")
        
        logger.info("‚úÖ Extracci√≥n completada exitosamente")
        
        return metricas['generacion'], metricas['precio'], metricas['volumen'], metricas['capacidad']
        
    except Exception as e:
        logger.error(f"‚ùå Error en extracci√≥n de datos XM: {str(e)}")
        raise


def procesar_datos(gene_df, precio_df, volumen_df, capacidad_df):
    """Procesa y combina datos de XM"""
    try:
        logger.info("üîß Procesando y combinando datos...")
        
        # Validar DataFrames de entrada
        for nombre, df in [('Generaci√≥n', gene_df), ('Precio', precio_df), 
                          ('Volumen', volumen_df), ('Capacidad', capacidad_df)]:
            if df is None or len(df) == 0:
                raise ValueError(f"DataFrame de {nombre} vac√≠o")
            if 'Date' not in df.columns:
                raise ValueError(f"DataFrame de {nombre} sin columna Date")
        
        # Funci√≥n para agregar valores horarios (columnas Values_HourXX)
        def agregar_valores_horarios(df, metrica_nombre):
            """Suma las 24 horas para obtener el total diario"""
            columnas_horarias = [col for col in df.columns if col.startswith('Values_Hour')]
            
            if not columnas_horarias:
                raise ValueError(f"No se encontraron columnas horarias en {metrica_nombre}")
            
            logger.info(f"   üìä Agregando {len(columnas_horarias)} horas para {metrica_nombre}")
            
            # Sumar todas las horas (convertir a num√©rico primero por si acaso)
            for col in columnas_horarias:
                df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # Total diario = suma de las 24 horas
            df['Values'] = df[columnas_horarias].sum(axis=1)
            
            return df[['Date', 'Values']]
        
        # Funci√≥n para promediar valores horarios (para precios)
        def promediar_valores_horarios(df, metrica_nombre):
            """Promedia las 24 horas para obtener el precio promedio diario"""
            columnas_horarias = [col for col in df.columns if col.startswith('Values_Hour')]
            
            if not columnas_horarias:
                raise ValueError(f"No se encontraron columnas horarias en {metrica_nombre}")
            
            logger.info(f"   üìä Promediando {len(columnas_horarias)} horas para {metrica_nombre}")
            
            # Convertir a num√©rico
            for col in columnas_horarias:
                df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # Promedio diario de las 24 horas
            df['Values'] = df[columnas_horarias].mean(axis=1)
            
            return df[['Date', 'Values']]
        
        # Procesar cada m√©trica
        logger.info("   üîÑ Procesando generaci√≥n (suma horaria)...")
        gene_df = agregar_valores_horarios(gene_df, 'Generaci√≥n')
        
        logger.info("   üîÑ Procesando precio (promedio horario)...")
        precio_df = promediar_valores_horarios(precio_df, 'Precio')
        
        # Los datos de embalses ya son diarios (columna 'Value' en lugar de horarias)
        logger.info("   üîÑ Procesando volumen embalses (ya es diario)...")
        if 'Value' in volumen_df.columns:
            volumen_df = volumen_df.rename(columns={'Value': 'Values'})[['Date', 'Values']]
        else:
            volumen_df = agregar_valores_horarios(volumen_df, 'Volumen')
        
        logger.info("   üîÑ Procesando capacidad embalses (ya es diario)...")
        if 'Value' in capacidad_df.columns:
            capacidad_df = capacidad_df.rename(columns={'Value': 'Values'})[['Date', 'Values']]
        else:
            capacidad_df = agregar_valores_horarios(capacidad_df, 'Capacidad')
        
        # Calcular porcentaje de embalses
        logger.info("   üìä Calculando porcentaje de embalses...")
        embalse_df = pd.merge(
            volumen_df[['Date', 'Values']], 
            capacidad_df[['Date', 'Values']], 
            on='Date', 
            suffixes=('_vol', '_cap'),
            how='inner'
        )
        
        # Evitar divisi√≥n por cero
        embalse_df['PorcentajeEmbalse'] = (
            (embalse_df['Values_vol'] / embalse_df['Values_cap'].replace(0, pd.NA)) * 100
        )
        embalse_df = embalse_df[['Date', 'PorcentajeEmbalse']]
        
        # Combinar todas las m√©tricas
        logger.info("   üîÄ Combinando m√©tricas...")
        df_combined = pd.merge(
            gene_df[['Date', 'Values']], 
            precio_df[['Date', 'Values']], 
            on='Date', 
            suffixes=('_gene', '_precio'),
            how='inner'
        )
        
        df_combined = pd.merge(
            df_combined, 
            embalse_df, 
            on='Date',
            how='inner'
        )
        
        # Convertir generaci√≥n de Wh a GWh (como en el ETL)
        df_combined['Values_gene'] = df_combined['Values_gene'] / 1_000_000
        
        # Renombrar columnas con nombres descriptivos
        df_combined.rename(columns={
            'Values_gene': 'Generacion_GWh',
            'Values_precio': 'PrecioBolsa_COP_kWh',
            'Date': 'Fecha'
        }, inplace=True)
        
        # Limpiar valores nulos e infinitos
        df_combined = df_combined.replace([float('inf'), float('-inf')], pd.NA)
        df_combined = df_combined.dropna()
        
        # Convertir Fecha a datetime si no lo es
        if not pd.api.types.is_datetime64_any_dtype(df_combined['Fecha']):
            df_combined['Fecha'] = pd.to_datetime(df_combined['Fecha'])
        
        # Ordenar por fecha
        df_combined = df_combined.sort_values('Fecha')
        
        # Agregar columna de timestamp actualizado
        df_combined['FechaActualizacion'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # Validar resultado final
        if len(df_combined) == 0:
            raise ValueError("No hay datos despu√©s del procesamiento")
        
        logger.info(f"‚úÖ Procesamiento completado: {len(df_combined)} registros v√°lidos")
        logger.info(f"   üìÖ Rango de fechas: {df_combined['Fecha'].min()} a {df_combined['Fecha'].max()}")
        logger.info(f"   üìà Generaci√≥n promedio: {df_combined['Generacion_GWh'].mean():.2f} GWh")
        logger.info(f"   üí∞ Precio promedio: {df_combined['PrecioBolsa_COP_kWh'].mean():.2f} COP/kWh")
        logger.info(f"   üåä Embalses promedio: {df_combined['PorcentajeEmbalse'].mean():.2f}%")
        
        return df_combined
        
    except Exception as e:
        logger.error(f"‚ùå Error procesando datos: {str(e)}")
        raise


def crear_spatial_dataframe(df_combined):
    """Convierte DataFrame a Spatial DataFrame con geometr√≠a"""
    try:
        logger.info("üó∫Ô∏è  Creando Spatial DataFrame...")
        
        # Agregar columnas de coordenadas
        df_combined['X'] = BOGOTA_COORDS['x']
        df_combined['Y'] = BOGOTA_COORDS['y']
        
        # Crear geometr√≠a Point para cada registro
        geometry = [Point(BOGOTA_COORDS) for _ in range(len(df_combined))]
        
        # Crear Spatial DataFrame usando GeoAccessor
        sedf = pd.DataFrame.spatial.from_xy(
            df=df_combined,
            x_column='X',
            y_column='Y',
            sr=4326
        )
        
        logger.info(f"‚úÖ Spatial DataFrame creado con {len(sedf)} features")
        
        return sedf
        
    except Exception as e:
        logger.error(f"‚ùå Error creando Spatial DataFrame: {str(e)}")
        raise


def verificar_feature_layer_existe(gis, layer_id):
    """Verifica si existe el CSV Item"""
    try:
        if not layer_id:
            return False
        
        item = gis.content.get(layer_id)
        if item and item.type == 'CSV':
            logger.info(f"‚úì CSV Item encontrado: {item.title}")
            return True
        else:
            logger.warning(f"‚ö†Ô∏è  ID {layer_id} no corresponde a un CSV v√°lido")
            return False
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  No se pudo verificar CSV Item: {e}")
        return False


def publicar_nuevo_feature_layer(gis, sedf):
    """Publica un nuevo CSV Layer en ArcGIS Enterprise (compatible con cuentas est√°ndar)"""
    import tempfile
    import os
    
    try:
        logger.info("üì§ Publicando datos a ArcGIS Enterprise como CSV...")
        
        # Crear archivo CSV temporal
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8') as tmp:
            csv_path = tmp.name
            # Convertir DataFrame a CSV (sin la columna geometry)
            df_export = sedf.copy()
            # Agregar columnas de latitud y longitud
            df_export['Latitude'] = df_export['SHAPE'].apply(lambda x: x.y if hasattr(x, 'y') else None)
            df_export['Longitude'] = df_export['SHAPE'].apply(lambda x: x.x if hasattr(x, 'x') else None)
            # Eliminar columna SHAPE
            df_export = df_export.drop(columns=['SHAPE'])
            df_export.to_csv(tmp.name, index=False)
        
        logger.info(f"   ‚úì CSV temporal creado: {csv_path}")
        
        # Publicar solo CSV como Item (sin Feature Service)
        item_properties = {
            'title': 'Metricas Energia XM Colombia',
            'description': 'Datos de generaci√≥n, precio de bolsa y porcentaje de embalses del Sistema El√©ctrico Colombiano (fuente: XM). Incluye coordenadas para visualizaci√≥n en mapas.',
            'tags': 'energia, XM, Colombia, dashboard, generacion, precio, embalses, melissa, CSV',
            'type': 'CSV'
        }
        
        csv_item = gis.content.add(item_properties, data=csv_path)
        logger.info(f"   ‚úì CSV publicado ID: {csv_item.id}")
        
        # Limpiar archivo temporal
        os.unlink(csv_path)
        
        logger.info("‚úÖ CSV Layer publicado exitosamente!")
        logger.info(f"   üìç T√≠tulo: {csv_item.title}")
        logger.info(f"   üîó URL: {ARCGIS_PORTAL_URL}home/item.html?id={csv_item.id}")
        logger.info(f"   üÜî Item ID: {csv_item.id}")
        logger.info("")
        logger.info("=" * 70)
        logger.info("‚ö†Ô∏è  ACCI√ìN REQUERIDA: Guarda este ID para futuras actualizaciones")
        logger.info("=" * 70)
        logger.info("")
        logger.info(f"Agrega esta l√≠nea a tu archivo .env:")
        logger.info(f"FEATURE_LAYER_ID={csv_item.id}")
        logger.info("")
        logger.info(f"O actualiza la variable en el script:")
        logger.info(f'FEATURE_LAYER_ID = "{csv_item.id}"')
        logger.info("=" * 70)
        logger.info("")
        logger.info("üí° NOTA: Este CSV puede visualizarse en mapas de ArcGIS Enterprise")
        logger.info("         agregando las columnas Latitude y Longitude como coordenadas.")
        logger.info("")
        
        return csv_item
        
    except Exception as e:
        logger.error(f"‚ùå Error publicando Feature Layer: {str(e)}")
        raise


def actualizar_feature_layer(gis, layer_id, sedf):
    """Actualiza un CSV Item existente con nuevos datos"""
    import tempfile
    import os
    
    try:
        logger.info("üîÑ Actualizando CSV Item existente...")
        
        # Obtener el Item
        item = gis.content.get(layer_id)
        if not item:
            raise ValueError(f"No se encontr√≥ Item con ID: {layer_id}")
        
        logger.info(f"   üìç T√≠tulo: {item.title}")
        logger.info(f"   üîó URL: {ARCGIS_PORTAL_URL}home/item.html?id={item.id}")
        
        # Crear archivo CSV temporal con nuevos datos
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8') as tmp:
            csv_path = tmp.name
            # Convertir DataFrame a CSV
            df_export = sedf.copy()
            # Agregar coordenadas
            df_export['Latitude'] = df_export['SHAPE'].apply(lambda x: x.y if hasattr(x, 'y') else None)
            df_export['Longitude'] = df_export['SHAPE'].apply(lambda x: x.x if hasattr(x, 'x') else None)
            # Eliminar columna SHAPE
            df_export = df_export.drop(columns=['SHAPE'])
            df_export.to_csv(tmp.name, index=False)
        
        logger.info(f"   ‚úì CSV temporal creado con {len(df_export)} registros")
        
        # Actualizar Item con nuevos datos
        logger.info("   üì§ Subiendo nuevos datos...")
        update_result = item.update(data=csv_path)
        
        # Limpiar archivo temporal
        os.unlink(csv_path)
        
        if update_result:
            logger.info(f"‚úÖ CSV actualizado exitosamente con {len(sedf)} registros")
        else:
            raise Exception("La actualizaci√≥n fall√≥")
        
        return item
        
    except Exception as e:
        logger.error(f"‚ùå Error actualizando Feature Layer: {str(e)}")
        raise


def guardar_csv_local(sedf):
    """Guarda una copia permanente del CSV en el servidor (sin backups)"""
    try:
        logger.info("üíæ Guardando CSV en el servidor...")
        
        # Preparar DataFrame para exportar
        df_export = sedf.copy()
        df_export['Latitude'] = df_export['SHAPE'].apply(lambda x: x.y if hasattr(x, 'y') else None)
        df_export['Longitude'] = df_export['SHAPE'].apply(lambda x: x.x if hasattr(x, 'x') else None)
        df_export = df_export.drop(columns=['SHAPE'])
        
        # Guardar CSV principal (reemplaza el existente)
        df_export.to_csv(CSV_OUTPUT_FILE, index=False, encoding='utf-8')
        
        # Mostrar informaci√≥n del archivo
        file_size = CSV_OUTPUT_FILE.stat().st_size / 1024  # KB
        logger.info(f"   ‚úì CSV actualizado: {CSV_OUTPUT_FILE}")
        logger.info(f"   üìä Tama√±o: {file_size:.2f} KB")
        logger.info(f"   üìù Registros: {len(df_export)}")
        logger.info("‚úÖ CSV actualizado exitosamente")
        
    except Exception as e:
        logger.error(f"‚ùå Error guardando CSV local: {str(e)}")
        # No fallar el proceso completo si solo falla el guardado local
        logger.warning("‚ö†Ô∏è  Continuando con la actualizaci√≥n de ArcGIS...")


def limpiar_backups_antiguos(dias=30):
    """Elimina backups m√°s antiguos que X d√≠as"""
    try:
        import time
        tiempo_limite = time.time() - (dias * 86400)
        
        archivos_eliminados = 0
        for backup_file in CSV_BACKUP_DIR.glob("metricas_xm_*.csv"):
            if backup_file.stat().st_mtime < tiempo_limite:
                backup_file.unlink()
                archivos_eliminados += 1
        
        if archivos_eliminados > 0:
            logger.info(f"   üóëÔ∏è  Backups antiguos eliminados: {archivos_eliminados}")
            
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Error limpiando backups: {str(e)}")


def publicar_o_actualizar(gis, sedf, dry_run=False):
    """Publica nuevo Feature Layer o actualiza existente"""
    try:
        # Primero guardar copia local (siempre, incluso en dry-run)
        guardar_csv_local(sedf)
        
        if dry_run:
            logger.info("üîç MODO DRY-RUN: No se realizar√°n cambios en ArcGIS")
            logger.info(f"   Se publicar√≠an/actualizar√≠an {len(sedf)} registros")
            return None
        
        logger.info(f"üîç FEATURE_LAYER_ID configurado: {FEATURE_LAYER_ID}")
        
        if FEATURE_LAYER_ID and verificar_feature_layer_existe(gis, FEATURE_LAYER_ID):
            # Actualizar Feature Layer existente
            return actualizar_feature_layer(gis, FEATURE_LAYER_ID, sedf)
        else:
            # Publicar nuevo Feature Layer
            if FEATURE_LAYER_ID:
                logger.warning("‚ö†Ô∏è  FEATURE_LAYER_ID configurado pero no v√°lido. Creando nuevo...")
            else:
                logger.info("üìù No hay FEATURE_LAYER_ID configurado. Creando nuevo...")
            return publicar_nuevo_feature_layer(gis, sedf)
            
    except Exception as e:
        logger.error(f"‚ùå Error en publicaci√≥n/actualizaci√≥n: {str(e)}")
        raise


def main():
    """Funci√≥n principal"""
    parser = argparse.ArgumentParser(
        description='Actualiza datos de XM en ArcGIS Online'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Ejecuta sin publicar cambios (solo prueba)'
    )
    args = parser.parse_args()
    
    try:
        logger.info("=" * 70)
        logger.info("   ACTUALIZACI√ìN DE DATOS XM EN ARCGIS ENTERPRISE - MINENERGIA")
        logger.info("=" * 70)
        logger.info(f"Fecha/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"Portal: {ARCGIS_PORTAL_URL}")
        logger.info(f"Usuario ArcGIS: {ARCGIS_USERNAME}")
        logger.info(f"Modo: {'DRY-RUN (Prueba)' if args.dry_run else 'PRODUCCI√ìN'}")
        logger.info(f"D√≠as de historia: {DIAS_ATRAS}")
        logger.info("=" * 70)
        
        # Validar credenciales
        validar_credenciales()
        
        # Verificar si hay datos nuevos disponibles (optimizaci√≥n para ejecuci√≥n horaria)
        hay_datos_nuevos, ultima_fecha = verificar_datos_nuevos()
        
        if not hay_datos_nuevos:
            logger.info("=" * 70)
            logger.info(f"‚úÖ CSV ya actualizado hasta {ultima_fecha}. No se requiere actualizaci√≥n.")
            logger.info("üí° XM a√∫n no ha publicado datos m√°s recientes.")
            logger.info("   El script volver√° a verificar en la pr√≥xima ejecuci√≥n horaria.")
            logger.info("=" * 70)
            return 0
        
        # 1. Extraer datos de XM
        gene_df, precio_df, volumen_df, capacidad_df = extraer_datos_xm()
        
        # 2. Procesar y combinar datos
        df_combined = procesar_datos(gene_df, precio_df, volumen_df, capacidad_df)
        
        # 3. Crear Spatial DataFrame
        sedf = crear_spatial_dataframe(df_combined)
        
        # 4. Conectar a ArcGIS Enterprise
        logger.info(f"üîå Conectando a ArcGIS Enterprise: {ARCGIS_PORTAL_URL}")
        gis = GIS(url=ARCGIS_PORTAL_URL, username=ARCGIS_USERNAME, password=ARCGIS_PASSWORD)
        logger.info(f"‚úÖ Conectado como: {gis.properties.user.username}")
        
        # 5. Publicar o actualizar datos (CSV Item en el portal)
        publicar_o_actualizar(gis, sedf, dry_run=args.dry_run)
        
        # 6. Actualizar la CAPA HOSPEDADA (Feature Service) para que dashboards vean datos nuevos
        #    Esto es necesario porque el Feature Service est√° desacoplado del CSV:
        #    actualizar el CSV no actualiza la capa ni los dashboards.
        if not args.dry_run:
            try:
                from actualizar_capa_hospedada import actualizar_capa_hospedada
                hosted_id = os.getenv("HOSTED_LAYER_ITEM_ID", "")
                if hosted_id:
                    logger.info("üîÑ Actualizando capa hospedada (Feature Service)...")
                    actualizar_capa_hospedada(csv_path=str(CSV_OUTPUT_FILE))
                    logger.info("‚úÖ Capa hospedada actualizada ‚Äî dashboards ver√°n datos nuevos.")
                else:
                    logger.info("‚ÑπÔ∏è  HOSTED_LAYER_ITEM_ID no configurado. Solo se actualiz√≥ el CSV.")
                    logger.info("   Configura HOSTED_LAYER_ITEM_ID en .env para actualizar la capa hospedada.")
            except ImportError:
                logger.warning("‚ö†Ô∏è  No se encontr√≥ actualizar_capa_hospedada.py. Solo se actualiz√≥ el CSV.")
            except Exception as e:
                logger.error(f"‚ö†Ô∏è  Error actualizando capa hospedada: {e}")
                logger.error("   El CSV se actualiz√≥ correctamente, pero la capa hospedada no.")
        
        logger.info("=" * 70)
        logger.info("üéâ PROCESO COMPLETADO EXITOSAMENTE")
        logger.info("=" * 70)
        
        return 0
        
    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è  Proceso interrumpido por el usuario")
        return 130
        
    except Exception as e:
        logger.error("=" * 70)
        logger.error(f"üí• ERROR CR√çTICO: {str(e)}")
        logger.error("=" * 70)
        import traceback
        logger.error(traceback.format_exc())
        return 1


if __name__ == "__main__":
    sys.exit(main())
