import pandas as pd
from datetime import date, timedelta
from typing import Iterable, List, Tuple, Optional
import time

def chunk_date_ranges(start: date, end: date, chunk_days: int = 30) -> List[Tuple[date, date]]:
	"""Divide un rango [start, end] en sub-rangos de hasta chunk_days d√≠as (incluidos).
	Retorna lista de tuplas (ini, fin) contiguas y no superpuestas.
	"""
	if start > end:
		return []
	ranges = []
	cur = start
	while cur <= end:
		seg_end = min(cur + timedelta(days=chunk_days - 1), end)
		ranges.append((cur, seg_end))
		cur = seg_end + timedelta(days=1)
	return ranges

def fetch_gene_recurso_chunked(objetoAPI, start: date, end: date, filtros: Iterable[str], batch_size: int = 50, chunk_days: int = 180, retries: int = 2, backoff_sec: float = 0.8) -> pd.DataFrame:
	"""Consulta Gene con Entity='Recurso' para una lista de filtros (SIC) en lotes y por chunks de fechas.
	Devuelve DataFrame con columnas: ['Codigo','Fecha','Generacion_GWh'] agregadas por d√≠a.
	OPTIMIZADO: Usa cache manager para evitar consultas repetidas a API.
	MEJORA DE PERFORMANCE: chunk_days din√°mico seg√∫n tama√±o del rango.
	"""
	from utils._xm import fetch_metric_data
	from utils.cache_manager import get_cache_key, get_from_cache, save_to_cache
	import logging
	logger = logging.getLogger(__name__)
	
	filtros = [str(x).strip() for x in filtros if x and isinstance(x, (str, int))]
	if not filtros:
		return pd.DataFrame(columns=['Codigo','Fecha','Generacion_GWh'])

	# OPTIMIZACI√ìN: Cachear resultado completo de consulta
	filtros_hash = hash(tuple(sorted(filtros)))
	cache_key = get_cache_key('gene_recurso_chunked', filtros_hash, start, end)
	cached_data = get_from_cache(cache_key, allow_expired=False)
	if cached_data is not None:
		logger.info(f"‚úÖ Cache v√°lido para Gene/Recurso ({len(filtros)} c√≥digos, {start} a {end})")
		return cached_data

	# OPTIMIZACI√ìN V2: Chunk days din√°mico seg√∫n rango total
	total_days = (end - start).days
	if total_days <= 60:
		chunk_days = total_days  # 1 consulta para rangos cortos
		logger.info(f"üìä Rango corto ({total_days} d√≠as) - 1 consulta")
	elif total_days <= 180:
		chunk_days = 90  # 2 consultas para rango medio
		logger.info(f"üìä Rango medio ({total_days} d√≠as) - ~{(total_days//90)+1} consultas")
	elif total_days <= 365:
		chunk_days = 180  # 2-3 consultas para 1 a√±o
		logger.info(f"üìä Rango grande ({total_days} d√≠as) - ~{(total_days//180)+1} consultas")
	else:
		chunk_days = 365  # Max 365 d√≠as por chunk para rangos muy grandes
		logger.info(f"üìä Rango muy grande ({total_days} d√≠as) - ~{(total_days//365)+1} consultas")
	
	# OPTIMIZACI√ìN V3: Reducir batch_size para rangos grandes (evitar timeouts)
	if total_days > 365:
		batch_size = 30  # Lotes m√°s peque√±os para rangos >1 a√±o
		backoff_sec = 1.2  # M√°s pausa entre requests
		logger.info(f"‚ö†Ô∏è Rango >1 a√±o: batch_size reducido a {batch_size} para estabilidad")
	elif total_days > 180:
		batch_size = 40  # Lotes medianos para rangos >6 meses
		backoff_sec = 1.0
	
	import time
	registros = []
	total_batches = sum(1 for _ in chunk_date_ranges(start, end, chunk_days=chunk_days) for _ in range(0, len(filtros), batch_size))
	batch_count = 0
	
	for ini, fin in chunk_date_ranges(start, end, chunk_days=chunk_days):
		# Batches por c√≥digos SIC
		for i in range(0, len(filtros), batch_size):
			batch_count += 1
			lote = filtros[i:i+batch_size]
			
			# OPTIMIZACI√ìN: Backoff entre batches para evitar saturar API
			if batch_count > 1:
				time.sleep(backoff_sec)
			
			# Log de progreso para rangos grandes
			if total_batches > 5 and batch_count % 5 == 0:
				logger.info(f"üìä Progreso: {batch_count}/{total_batches} batches completados")
			
			# Usar la API directamente con c√≥digos espec√≠ficos
			df = objetoAPI.request_data("Gene", "Recurso", ini, fin, lote)
			if df is None or df.empty:
				continue
			horas_cols = [c for c in df.columns if str(c).startswith('Values_Hour')]
			if not horas_cols:
				continue
			# Identificar columna de c√≥digo en respuesta Gene (puede variar)
			code_col = None
			for cand in ('Values_code', 'Values_Code', 'Values_resourceCode', 'Values_ResourceCode'):
				if cand in df.columns:
					code_col = cand
					break
			for _, row in df.iterrows():
				try:
					kwh = sum(float(row.get(c)) for c in horas_cols if pd.notna(row.get(c)))
				except Exception:
					kwh = 0.0
				registros.append({
					'Codigo': str(row.get(code_col, '') if code_col else '').strip(),
					'Fecha': row.get('Date'),
					'Generacion_GWh': kwh/1_000_000.0
				})

	if not registros:
		return pd.DataFrame(columns=['Codigo','Fecha','Generacion_GWh'])
	
	df_out = pd.DataFrame(registros)
	# Asegurar tipos/orden b√°sico
	if 'Fecha' in df_out.columns:
		try:
			df_out['Fecha'] = pd.to_datetime(df_out['Fecha']).dt.date
		except Exception:
			pass
	
	# Cachear resultado por 6 horas (datos de generaci√≥n actualizados diariamente)
	save_to_cache(cache_key, df_out, cache_type='gene_recurso')
	logger.info(f"‚úÖ Cacheado Gene/Recurso: {len(df_out)} registros ({len(filtros)} c√≥digos)")
	
	return df_out

